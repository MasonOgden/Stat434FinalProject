---
title: "Modeling"
author: "Mason Ogden"
date: "6/1/2021"
output: html_document
---

### Setup

[Link](https://www.kaggle.com/c/shelter-animal-outcomes/data) to Kaggle competition

**Packages**

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(magrittr)
library(tidymodels)
library(baguette)
library(rules)
library(discrim)
library(fastDummies)

set.seed(143)
```

**Data**

```{r, message = FALSE, warning = FALSE}
# define working directory
dir <- here::here()

# directory where preprocessed data is held
preproc_dir <- dir %>%
  paste0("/preprocessed_data")

# read in preprocessed training data
train_small <- preproc_dir %>%
  paste0("/train_preprocessed.csv") %>%
  read_csv() %>%
  # drop rows with missing values (we only lose 1409 rows)
  drop_na() %>%
  mutate(
    animal_type = factor(animal_type),
    sex = factor(sex),
    color = factor(color),
    outcome = factor(outcome),
    outcome_month = factor(outcome_month),
    outcome_year = factor(outcome_year),
    mix = factor(mix)
  ) %>%
  slice_sample(n = 2000)

train_cv <- vfold_cv(train_small, v = 5, strata = outcome)
```

### Data Preprocessing

```{r}
animal_rec <- recipe(outcome ~ ., data = train_small) %>%
  update_role(animal_id, new_role = "ID") %>%
  step_normalize(age) %>%
  step_nzv(mix) %>%
  step_dummy(all_nominal_predictors())
  #step_nzv(contains("animal_type"),
  #         contains("sex"), contains("color"),
  #         contains("outcome_month"),
  #         contains("outcome_year"))

animal_trained <- animal_rec %>% prep(train_small)
train_proc <- animal_trained %>% bake(train_small)

class_spec_names <- c()
class_spec_list <- vector(mode= "list", length = 4)

lda_spec_names <- c()
lda_spec_list <- vector(mode = "list", length = 1)

other_spec_names <- c()
other_spec_list <- vector(mode = "list", length = 3)

svm_spec_names <- c()
svm_spec_list <- vector(mode = "list", length = 2)

rule_spec_names <- c()
rule_spec_list <- vector(mode = "list", length = 2)
```


### Initial Modeling

#### Specifying the models

[Link](https://www.tidymodels.org/find/parsnip/) to list of Tidymodels models
  
1. Bagged Decision Tree
  + engine: "rpart"
  + bag_tree()
  
```{r}
bag_dtree <- bag_tree() %>%
  set_engine("rpart", times = 3) %>%
  set_mode("classification")

class_spec_list[[1]] <- bag_dtree
class_spec_names <- c(class_spec_names, "bagged decision tree")
```

  
2. Boosted Tree
  + engine: "xgboost"
  + boost_tree()
  
```{r}
boosted_tree <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

rule_spec_list[[1]] <- boosted_tree
rule_spec_names <- c(other_spec_names, "boosted tree")
```

  
3. Decision Tree
  + engine: "rpart"
  + decision_tree()
  
```{r}
dtree <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

class_spec_list[[2]] <- dtree
class_spec_names <- c(class_spec_names, "decision tree")
```

  
4. Multi-layer Perceptron
  + just a single layer neural network
  + engine: "keras"
  + mlp()
  
```{r}
mlperceptron <- mlp() %>%
  set_engine("keras") %>%
  set_mode("classification")

other_spec_list[[1]] <- mlperceptron
other_spec_names <- c(other_spec_names, "MLP")
```

  
5. KNN
  + engine: "kknn"
  + nearest_neighbor()
  
```{r}
knn <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

class_spec_list[[3]] <- knn
class_spec_names <- c(class_spec_names, "KNN")
```

  
7. Random Forest
  + engine: "randomForest"
  + rand_forest()
  
```{r}
random_forest <- rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("classification")

class_spec_list[[4]] <- random_forest
class_spec_names <- c(class_spec_names, "random forest")
```

  
8. Polynomial SVM
  + engine: "kernlab"
  + svm_poly()
  
```{r}
svm <- svm_poly() %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_spec_list[[1]] <- svm
svm_spec_names <- c(svm_spec_names, "svm")
```


9. RBF SVM
  + engine: "kernlab"
  + svm_rbf()
  
```{r}
svm_radial <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_spec_list[[2]] <- svm
svm_spec_names <- c(svm_spec_names, "svm rbf")
```

  
10. C5.0 Rule-based Classification
  + engine: "C5.0"
  + C5_rules()
  
```{r}
c5_rules <- C5_rules() %>%
  set_engine("C5.0") %>%
  set_mode("classification")

rule_spec_list[[2]] <- c5_rules
rule_spec_names <- c(rule_spec_names, "C5.0 rules")
```

  
11. Flexible Discriminant Model
  + engine: "earth"
  + discrim_flexible()
  
```{r}
flexible_lda <- discrim_flexible() %>%
  set_engine("earth") %>%
  set_mode("classification")

lda_spec_list[[1]] <- flexible_lda
lda_spec_names <- c(lda_spec_names, "flexible LDA")
```


12. Multinomial Regression
  + engine: "glmnet"
  + multinom_reg()
  
```{r}
mlr <- multinom_reg() %>%
  set_engine("keras") %>%
  set_mode("classification")

other_spec_list[[2]] <- mlr
other_spec_names <- c(other_spec_names, "multinomial regression")
```

  
13. Naive Bayes
  + engine: "klaR"
  + naive_bayes()
  
```{r}
nbclassifier <- naive_Bayes() %>%
  set_engine("klaR") %>%
  set_mode("classification")

other_spec_list[[3]] <- nbclassifier
other_spec_names <- c(other_spec_names, "naive bayes")
```

  
14. Neural Network
  + made using keras from scratch
  + look up architectures for classification
  
#### Fitting Models with Cross-validation

```{r}
tibble(
  category = c("class", "lda", "svm", "rule", "other"),
  num_models = c(length(class_spec_names),
                 length(lda_spec_names),
                 length(svm_spec_names),
                 length(rule_spec_names),
                 length(other_spec_names)
                 ),
  models = c(paste0(class_spec_names, collapse = ", "),
             paste0(lda_spec_names, collapse = ", "),
             paste0(svm_spec_names, collapse = ", "),
             paste0(rule_spec_names, collapse = ", "),
             paste0(other_spec_names, collapse = ", ")
             )
)
```

**Models from Class**

```{r}
# none fail
number_to_run <- 1:4

class_cv_fitted_models <- class_spec_list[number_to_run] %>%
  map(\(x) workflow() %>%
        add_recipe(animal_rec) %>%
        add_model(x) %>%
        fit_resamples(train_cv))

class_cv_results <- map2_df(class_cv_fitted_models, class_spec_names[number_to_run],
        \(x, y) x %>% 
          collect_metrics() %>%
          add_column(model_name = y, .before = ".metric") %>%
          dplyr::select(model_name, metric = .metric, cv_value = mean)
        )

class_cv_results %>%
  ggplot(aes(x = model_name, y = cv_value, fill = metric)) + 
  geom_col(position = "dodge") + 
  coord_flip() + 
  labs(x = "", y = "Cross-validated Value",
       title = "Comparing Cross-validated Metrics of 4 Classification Models",
       subtitle = "Note: no hyperparameters have been tuned")
```

**LDA Models**

```{r}
number_to_run <- 1

lda_cv_fitted_models <- lda_spec_list[number_to_run] %>%
  map(\(x) workflow() %>%
        add_recipe(animal_rec) %>%
        add_model(x) %>%
        fit_resamples(train_cv))

lda_cv_results <- map2_df(lda_cv_fitted_models, lda_spec_names[number_to_run],
        \(x, y) x %>% 
          collect_metrics() %>%
          add_column(model_name = y, .before = ".metric") %>%
          dplyr::select(model_name, metric = .metric, cv_value = mean)
        )

lda_cv_results
```


**SVM models**

```{r}
# both run
number_to_run <- 1:2

svm_cv_fitted_models <- svm_spec_list[number_to_run] %>%
  map(\(x) workflow() %>%
        add_recipe(animal_rec) %>%
        add_model(x) %>%
        fit_resamples(train_cv))

svm_cv_results <- map2_df(svm_cv_fitted_models, svm_spec_names[number_to_run],
        \(x, y) x %>% 
          collect_metrics() %>%
          add_column(model_name = y, .before = ".metric") %>%
          dplyr::select(model_name, metric = .metric, cv_value = mean)
        )

svm_cv_results
```

**Rule-based models**

```{r}
# 2 runs but takes a while, 3 runs
number_to_run <- 1:2

rule_cv_fitted_models <- rule_spec_list[number_to_run] %>%
  map(\(x) workflow() %>%
        add_recipe(animal_rec) %>%
        add_model(x) %>%
        fit_resamples(train_cv))

rule_cv_results <- map2_df(rule_cv_fitted_models, rule_spec_names[number_to_run],
        \(x, y) x %>% 
          collect_metrics() %>%
          add_column(model_name = y, .before = ".metric") %>%
          dplyr::select(model_name, metric = .metric, cv_value = mean)
        )

rule_cv_results
```

**Other models**

```{r}
number_to_run <- 1:3

other_cv_fitted_models <- other_spec_list[number_to_run] %>%
  purrr::map(\(x) workflow() %>%
        add_recipe(animal_rec) %>%
        add_model(x) %>%
        fit_resamples(train_cv))

other_cv_results <- map2_df(other_cv_fitted_models, other_spec_names[number_to_run],
        \(x, y) x %>% 
          collect_metrics() %>%
          add_column(model_name = y, .before = ".metric") %>%
          dplyr::select(model_name, metric = .metric, cv_value = mean)
        )

other_cv_results
```

### Aggregating Baseline Model Results

```{r}
baseline_model_results <- rbind(class_cv_results, lda_cv_results, svm_cv_results, rule_cv_results, other_cv_results)

asc_rocauc_order <- baseline_model_results %>%
  filter(metric == "roc_auc") %>% 
  arrange(cv_value) %>% 
  pull(model_name)

baseline_model_results %>%
  mutate(model_name = factor(model_name, levels = asc_rocauc_order)) %>%
  ggplot(aes(x = model_name, y = cv_value, fill = metric)) + 
  geom_col(position = "dodge") + 
  coord_flip() + 
  labs(x = "", y = "Cross-validated Value",
       title = "Comparing Cross-validated Metrics of 12 Classification Models",
       subtitle = "Note: no hyperparameters have been tuned")
  
```

### Tuning Hyperparameters of Top 4 Models:

**Flexible LDA**

Wasn't able to tune `num_terms` because using anything less than all 58 columns results in matrix singularity issues. 

All pruning methods except "backward" and "none" were not compatible with anything but binary classification

```{r}
lda_flex_tune <- discrim_flexible(prod_degree = tune(),
                                  prune_method = tune()) %>%
  set_engine("earth") %>% 
  set_mode("classification")

lda_tune_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(lda_flex_tune)

lda_tune_grid <- expand.grid(prod_degree = c(1, 2), prune_method = c("backward", "none"))

lda_tune_gs <- tune_grid(
  lda_tune_wflow,
  resamples = train_cv,
  grid = lda_tune_grid
)

best_lda_attrs <- lda_tune_gs %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean) %>%
  slice_tail()

# value = 1
best_prod_degree <- best_lda_attrs %>% 
  pull(prod_degree)

# value = "backward"
best_prune_method <- best_lda_attrs %>%
  pull(prune_method)

best_lda_flex_spec <- discrim_flexible(prod_degree = best_prod_degree,
                                       prune_method = best_prune_method) %>%
  set_engine("earth") %>% 
  set_mode("classification")

best_lda_flex_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(best_lda_flex_spec)
```

**Decision Tree**

```{r}
dtree_tune <- decision_tree(cost_complexity = tune(),
                            tree_depth = tune(),
                            min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

dtree_tune_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(dtree_tune)

dtree_tune_grid <- grid_regular(cost_complexity(),
                                tree_depth(),
                                min_n(),
                                levels = 3)

dtree_tune_gs <- tune_grid(
  dtree_tune_wflow,
  resamples = train_cv,
  grid = dtree_tune_grid
)

best_dtree_attrs <- dtree_tune_gs %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean) %>%
  slice_tail()

# value = 3.162278e-06
best_cost_complexity <- best_dtree_attrs %>%
  pull(cost_complexity)

# value = 8
best_tree_depth <- best_dtree_attrs %>%
  pull(tree_depth)

# value = 40
best_min_n <- best_dtree_attrs %>%
  pull(min_n)

best_dtree_spec <- decision_tree(cost_complexity = best_cost_complexity,
                            tree_depth = best_tree_depth,
                            min_n = best_min_n) %>%
  set_engine("rpart") %>%
  set_mode("classification")

best_dtree_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(best_dtree_spec)
```

**Boosted Tree**

Tuning individual tree parameters:

```{r}
btree_tune1 <- boost_tree(mtry = tune(),
                          min_n = tune(),
                          tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

btree_tune1_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(btree_tune1)

btree_tune1_grid <- grid_regular(mtry(c(1, ncol(train_proc))),
                                 min_n(),
                                 tree_depth(),
                                 levels = 3
                                 )

btree_tune1_gs <- tune_grid(
  btree_tune1_wflow,
  resamples = train_cv,
  grid = btree_tune1_grid
)

best_btree_tune1 <- btree_tune1_gs %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean) %>%
  slice_tail()

# value = 58
best_mtry <- best_btree_tune1 %>%
  pull(mtry)

# value = 2
best_min_n <- best_btree_tune1 %>%
  pull(min_n)

# value = 8
best_tree_depth <- best_btree_tune1 %>%
  pull(tree_depth)
```

Tuning boosting parameters:

```{r}
btree_tune2 <- boost_tree(mtry = best_mtry,
                          min_n = best_min_n,
                          tree_depth = best_tree_depth,
                          trees = tune(),
                          learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

btree_tune2_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(btree_tune2)

btree_tune2_grid <- grid_regular(trees(),
                                 learn_rate(),
                                 levels = 2
                                 )

btree_tune2_gs <- tune_grid(
  btree_tune2_wflow,
  resamples = train_cv,
  grid = btree_tune2_grid
)

best_btree_tune2 <- btree_tune2_gs %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean) %>%
  slice_tail()

# value = 2000 
best_num_trees <- best_btree_tune2 %>%
  pull(trees)

# value = 0.1
best_learn_rate <- best_btree_tune2 %>%
  pull(learn_rate)

best_btree_spec <- boost_tree(mtry = best_mtry,
                              min_n = best_min_n,
                              tree_depth = best_tree_depth,
                              trees = best_num_trees,
                              learn_rate = best_learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
  
best_btree_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(best_btree_spec)
```


```{r}
rforest_tune <- rand_forest(mtry = tune(),
                            trees = tune(),
                            min_n = tune()) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

rforest_tune_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(rforest_tune)

rforest_tune_grid <- grid_regular(mtry(c(1, ncol(train_proc) - 2)),
                              dials::trees(),
                              min_n(),
                              levels = 2
                              )

rforest_tune_gs <- tune_grid(
  rforest_tune_wflow,
  resamples = train_cv,
  grid = rforest_tune_grid
)

best_rforest_attrs <- rforest_tune_gs %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean) %>%
  slice_tail()

# value = 58
best_mtry <- best_rforest_attrs %>%
  pull(mtry) %>%
  subtract(2)

# value = 2000
best_trees <- best_rforest_attrs %>%
  pull(trees)

# value = 40
best_min_n <- best_rforest_attrs %>%
  pull(min_n)

best_rforest_spec <- rand_forest(mtry = best_mtry,
                                 trees = best_trees,
                                 min_n = best_min_n) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

best_rforest_wflow <- workflow() %>%
  add_recipe(animal_rec) %>%
  add_model(best_rforest_spec)
```

### Comparing Tuned Models:

I may need to reduce 'trees' argument, since that seems to be getting set to the max value of 2000, which is unreasonable

```{r}
tuned_model_names <- c("flexible LDA", "decision tree", "boosted tree", "random forest")
tuned_wflow <- list(best_lda_flex_wflow, best_dtree_wflow, best_btree_wflow, best_rforest_wflow)

eval_metrics <- metric_set(roc_auc, accuracy, precision, recall, f_meas)

tuned_wflow_cvfit <- tuned_wflow %>%
  map(\(x) x %>% fit_resamples(resamples = train_cv,
                               metrics = eval_metrics))

tuned_cv_results <- map2_df(tuned_wflow_cvfit, tuned_model_names,
                            \(x, y) x %>% 
                              collect_metrics() %>%
                              add_column(model_name = y, .before = ".metric") %>%
                              dplyr::select(model_name, metric = .metric, cv_value = mean)
                            )

asc_rocauc_order <- tuned_cv_results %>%
  filter(metric == "roc_auc") %>% 
  arrange(cv_value) %>% 
  pull(model_name)

tuned_cv_results %>%
  mutate(model_name = factor(model_name, levels = asc_rocauc_order)) %>%
  ggplot(aes(x = model_name, y = cv_value, fill = metric)) + 
  geom_col(position = "dodge") + 
  coord_flip() + 
  labs(x = "", y = "Cross-validated Value",
       title = "Comparing Cross-validated Metrics of 4 Classification Models",
       subtitle = "Note: All hyperparameters have been tuned")
```

### Fitting Chosen Model to All Training Data

**Get all training data and testing data**

```{r}
train <- preproc_dir %>%
  paste0("/train_preprocessed.csv") %>%
  read_csv() %>%
  drop_na() %>%
  mutate(
    animal_type = factor(animal_type),
    sex = factor(sex),
    color = factor(color),
    outcome = factor(outcome),
    outcome_month = factor(outcome_month),
    outcome_year = factor(outcome_year),
    mix = factor(mix)
  )

test <- preproc_dir %>%
  paste0("/test_preprocessed.csv") %>%
  read_csv() %>%
  drop_na() %>%
  mutate(
    animal_id = as.character(animal_id),
    animal_type = factor(animal_type),
    sex = factor(sex),
    color = factor(color),
    outcome_month = factor(outcome_month),
    outcome_year = factor(outcome_year),
    mix = factor(mix)
  )
```


```{r}
final_fit <- best_lda_flex_wflow %>%
  fit(train)
```

### Creating Submission File

```{r}
submission_dir <- dir %>%
  paste0("/submissions")

pred_outcomes <- final_fit %>%
  predict(new_data = test, type = "class")

submission <- pred_outcomes %>%
  add_column(ID = 1:nrow(pred_outcomes)) %>%
  select(ID, outcome = .pred_class) %>%
  dummy_cols() %>%
  select(-outcome, ID,
         Adoption = outcome_Adoption,
         Died = outcome_Died,
         Euthanasia = outcome_Euthanasia,
         Return_to_owner = outcome_Return_to_owner,
         Transfer = outcome_Transfer)

submission %>%
  write_csv(file = paste0(submission_dir, "/submission1.csv"))
```

